{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"owl-with-odds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651\n",
      "538\n",
      "536\n"
     ]
    }
   ],
   "source": [
    "subset = ['t1_odds', 't2_odds']\n",
    "print(len(df))\n",
    "df.dropna(subset=subset ,inplace=True)\n",
    "print(len(df))\n",
    "\n",
    "df['t1_odds'] = pd.to_numeric(df['t1_odds'], errors='coerce')\n",
    "df['t2_odds'] = pd.to_numeric(df['t2_odds'], errors='coerce')\n",
    "df.dropna(subset=subset ,inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "476\n"
     ]
    }
   ],
   "source": [
    "#Split the validation set\n",
    "df_train = df[:-60]\n",
    "\n",
    "df_test = df[-60:]\n",
    "print(len(df_test))\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inclusive features\n",
    "features = ['corona_virus_isolation', 't1_wins_season',\n",
    "       't1_losses_season', 't2_wins_season', 't2_losses_season',\n",
    "       't1_matches_season', 't2_matches_season', 't1_win_percent_season',\n",
    "       't2_win_percent_season', 't1_wins_alltime', 't1_losses_alltime',\n",
    "       't2_wins_alltime', 't2_losses_alltime', 't1_matches_alltime',\n",
    "       't2_matches_alltime', 't1_win_percent_alltime',\n",
    "       't2_win_percent_alltime', 't1_wins_last_3', 't1_losses_last_3',\n",
    "       't2_wins_last_3', 't2_losses_last_3', 't1_win_percent_last_3',\n",
    "       't2_win_percent_last_3', 't1_wins_last_5', 't1_losses_last_5',\n",
    "       't2_wins_last_5', 't2_losses_last_5', 't1_win_percent_last_5',\n",
    "       't2_win_percent_last_5', 't1_wins_last_10', 't1_losses_last_10',\n",
    "       't2_wins_last_10', 't2_losses_last_10', 't1_win_percent_last_10',\n",
    "       't2_win_percent_last_10', \n",
    "        't1_wins_vs_t2', 't1_losses_vs_t2',\n",
    "       't1_matches_vs_t2', 't1_odds', 't2_odds', 'winner_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476, 41)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_filtered = df_test[features].copy()\n",
    "df_train_filtered = df_train[features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_filtered.dropna(inplace=True)\n",
    "df_train_filtered.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bet_ev(odds, prob):\n",
    "    if odds>0:\n",
    "        return ((odds * prob) - (100 * (1-prob)) )\n",
    "    else:\n",
    "        return ((100 / abs(odds))*100*prob - (100 * (1-prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bet_return(odds):\n",
    "    if odds>0:\n",
    "        return odds\n",
    "    else:\n",
    "        return (100 / abs(odds))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input DF must have these columns:\n",
    "#t1_odds (American)\n",
    "#t2_odds (American)\n",
    "#t1_prob (0->1)\n",
    "#t2_prob (0->1)\n",
    "#winner (0 or 1)\n",
    "\n",
    "\n",
    "\n",
    "def get_ev_from_df(ev_df, print_stats = False):\n",
    "    num_matches = 0\n",
    "    num_bets = 0\n",
    "    num_wins = 0\n",
    "    num_losses= 0\n",
    "    num_under= 0\n",
    "    num_under_losses = 0\n",
    "    num_under_wins = 0\n",
    "    num_even = 0\n",
    "    num_even_losses = 0\n",
    "    num_even_wins = 0\n",
    "    num_fav = 0\n",
    "    num_fav_wins = 0\n",
    "    num_fav_losses = 0\n",
    "    profit = 0\n",
    "    profit_per_bet = 0\n",
    "    profit_per_match = 0    \n",
    "\n",
    "    for index, row in ev_df.iterrows():\n",
    "        num_matches = num_matches+1\n",
    "        t1_bet_ev = get_bet_ev(row['t1_odds'], row['t1_prob'])\n",
    "        #print(f\"ODDS:{row['t1_odds']} PROB: {row['t1_prob']} EV: {t1_bet_ev}\")\n",
    "        t2_bet_ev = get_bet_ev(row['t2_odds'], row['t2_prob'])\n",
    "        #print(f\"ODDS:{row['t2_odds']} PROB: {row['t2_prob']} EV: {t2_bet_ev}\")\n",
    "        #print()\n",
    "        \n",
    "        t1_bet_return = get_bet_return(row['t1_odds'])\n",
    "        t2_bet_return = get_bet_return(row['t2_odds'])\n",
    "        \n",
    "        \n",
    "        if (t1_bet_ev > 0 or t2_bet_ev > 0):\n",
    "            num_bets = num_bets+1\n",
    "\n",
    "            \n",
    "        if t1_bet_ev > 0:\n",
    "            if row['winner'] == 0:\n",
    "                num_wins += 1\n",
    "                profit = profit + t1_bet_return\n",
    "                #print(t1_bet_return)\n",
    "            elif row['winner'] == 1:\n",
    "                num_losses += 1\n",
    "                profit = profit - 100\n",
    "            if (t1_bet_return > t2_bet_return):\n",
    "                num_under += 1\n",
    "                if row['winner'] == 0:\n",
    "                    num_under_wins += 1\n",
    "                elif row['winner'] == 1:\n",
    "                    num_under_losses += 1\n",
    "            elif (t1_bet_return < t2_bet_return):\n",
    "                num_fav += 1\n",
    "                if row['winner'] == 0:\n",
    "                    num_fav_wins += 1\n",
    "                elif row['winner'] == 1:\n",
    "                    num_fav_losses += 1\n",
    "            else:\n",
    "                num_even += 1\n",
    "                if row['winner'] == 0:\n",
    "                    num_even_wins += 1\n",
    "                elif row['winner'] == 1:\n",
    "                    num_even_losses += 1\n",
    "\n",
    "        if t2_bet_ev > 0:\n",
    "            if row['winner'] == 1:\n",
    "                num_wins += 1                    \n",
    "                profit = profit + t2_bet_return\n",
    "            elif row['winner'] == 0:\n",
    "                num_losses += 1\n",
    "                profit = profit - 100\n",
    "            if (t2_bet_return > t1_bet_return):\n",
    "                num_under += 1\n",
    "                if row['winner'] == 1:\n",
    "                    num_under_wins += 1\n",
    "                elif row['winner'] == 0:\n",
    "                    num_under_losses += 1\n",
    "            elif (t2_bet_return < t1_bet_return):\n",
    "                num_fav += 1\n",
    "                if row['winner'] == 1:\n",
    "                    num_fav_wins += 1\n",
    "                elif row['winner'] == 0:\n",
    "                    num_fav_losses += 1\n",
    "            else:\n",
    "                num_even += 1\n",
    "                if row['winner'] == 1:\n",
    "                    num_even_wins += 1\n",
    "                elif row['winner'] == 0:\n",
    "                    num_even_losses += 1\n",
    "            \n",
    "    profit_per_bet = profit / num_bets\n",
    "    profit_per_match = profit / num_matches\n",
    "        \n",
    "        \n",
    "    if print_stats:\n",
    "        print(f\"\"\"\n",
    "          Number of matches: {num_matches}\n",
    "          Number of bets: {num_bets}\n",
    "          Number of winning bets: {num_wins}\n",
    "          Number of losing bets: {num_losses}\n",
    "          Number of underdog bets: {num_under}\n",
    "          Number of underdog wins: {num_under_wins}\n",
    "          Number of underdog losses: {num_under_losses}\n",
    "          Number of Favorite bets: {num_fav}\n",
    "          Number of favorite wins: {num_fav_wins}\n",
    "          Number of favorite losses: {num_fav_losses}\n",
    "          Number of even bets: {num_even}\n",
    "          Number of even wins: {num_even_wins}\n",
    "          Number of even losses: {num_even_losses}\n",
    "          Profit: {profit}\n",
    "          Profit per bet: {profit_per_bet}\n",
    "          Profit per match: {profit_per_match}\n",
    "          \n",
    "          \"\"\")\n",
    "        \n",
    "    return (profit_per_bet)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input the train df and model and we will return a customer 5x cross validation score based off of expected value\n",
    "#t1_odds and t2_odd MUST be the last 2 columns or this will break.\n",
    "\n",
    "def custom_cv_eval(df, m):\n",
    "    #We need to split away the winner...\n",
    "    y = df['winner_label'].copy()\n",
    "    #display(y)\n",
    "    X = df.drop('winner_label', axis=1)\n",
    "    \n",
    "    ##We need a numpy array it seems like...\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    #display(X)\n",
    "    running_total = 0\n",
    "    count=1\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=75)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        #print(test_index)\n",
    "        model.fit(X_train, y_train)\n",
    "        probs=model.predict_proba(X_test)\n",
    "        #We need to prep the dataframe to evaluate....\n",
    "        #X_odds = X_test[['t1_odds', 't2_odds']]\n",
    "        #print(X_test)\n",
    "        #print(X_test[:, -1])\n",
    "        #print(X_test[:, -2])\n",
    "        X_odds = list(zip(X_test[:, -2], X_test[:, -1], probs[:, 0], probs[:, 1], y_test))\n",
    "        ev_prepped_df = pd.DataFrame(X_odds, columns=['t1_odds', 't2_odds', 't1_prob', 't2_prob', 'winner'])\n",
    "        #display(temp_df)\n",
    "        #print(f\"{count}: {get_ev_from_df(ev_prepped_df, print_stats = False)}\")\n",
    "        count=count+1\n",
    "        running_total = running_total + get_ev_from_df(ev_prepped_df, print_stats = False)\n",
    "    return running_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(max_depth=10, min_samples_leaf=3, random_state=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: -26.38405656100527\n"
     ]
    }
   ],
   "source": [
    "print(f\"total: {custom_cv_eval(df_train_filtered, model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Let's automate finding features!\n",
    "pos_features = ['corona_virus_isolation', 't1_wins_season',\n",
    "       't1_losses_season', 't2_wins_season', 't2_losses_season',\n",
    "       't1_matches_season', 't2_matches_season', 't1_win_percent_season',\n",
    "       't2_win_percent_season', 't1_wins_alltime', 't1_losses_alltime',\n",
    "       't2_wins_alltime', 't2_losses_alltime', 't1_matches_alltime',\n",
    "       't2_matches_alltime', 't1_win_percent_alltime',\n",
    "       't2_win_percent_alltime', 't1_wins_last_3', 't1_losses_last_3',\n",
    "       't2_wins_last_3', 't2_losses_last_3', 't1_win_percent_last_3',\n",
    "       't2_win_percent_last_3', 't1_wins_last_5', 't1_losses_last_5',\n",
    "       't2_wins_last_5', 't2_losses_last_5', 't1_win_percent_last_5',\n",
    "       't2_win_percent_last_5', 't1_wins_last_10', 't1_losses_last_10',\n",
    "       't2_wins_last_10', 't2_losses_last_10', 't1_win_percent_last_10',\n",
    "       't2_win_percent_last_10', \n",
    "        't1_wins_vs_t2', 't1_losses_vs_t2',\n",
    "       't1_matches_vs_t2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_features = ['winner_label', 't1_odds', 't2_odds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_features(features, model, df, current_features, scale=False):\n",
    "    best_feature = \"\"\n",
    "    winner_labels = df['winner_label'].copy()\n",
    "    initial_df = df[current_features]\n",
    "    #display(initial_df)\n",
    "    #display(winner_labels)\n",
    "    \n",
    "    best_score = custom_cv_eval(df[current_features], model)\n",
    "    best_feature = \"\"\n",
    "    \n",
    "    print(f\"Current best score is: {best_score}\")\n",
    "    for f in features:\n",
    "        if f not in current_features:\n",
    "            new_features = [f] + current_features\n",
    "            df_sel=df[new_features]\n",
    "            if scale == True:\n",
    "                sc = StandardScaler()\n",
    "                df_sel = sc.fit_transform(df_sel)\n",
    "            new_score = custom_cv_eval(df_sel, model)\n",
    "            #print(f\"Total score for {f} is: {new_score}\")\n",
    "            if new_score > best_score:\n",
    "                best_score = new_score\n",
    "                best_feature = f\n",
    "            #print()\n",
    "    #Keep running until we don't improve\n",
    "    if best_feature != \"\":\n",
    "        print(f\"The best feature was {best_feature}.  It scored {best_score}\")\n",
    "        current_features = [best_feature] + current_features\n",
    "        \n",
    "        return(get_best_features(features, model, df, current_features, scale))\n",
    "    else:\n",
    "        print(\"NO IMPROVEMENT\")\n",
    "        print(f\"FINAL BEST SCORE: {best_score}\")\n",
    "        return current_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current best score is: -30.218773001761235\n",
      "The best feature was t2_matches_alltime.  It scored 30.874776448625802\n",
      "Current best score is: 30.874776448625802\n",
      "The best feature was t1_wins_last_3.  It scored 74.52979394547228\n",
      "Current best score is: 74.52979394547228\n",
      "NO IMPROVEMENT\n",
      "FINAL BEST SCORE: 74.52979394547228\n"
     ]
    }
   ],
   "source": [
    "f=get_best_features(pos_features, model,  df_train_filtered, current_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t1_wins_last_3', 't2_matches_alltime', 'winner_label', 't1_odds', 't2_odds']\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.52979394547228"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "custom_cv_eval(df_train_filtered[f], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try linear regression\n",
    "model = LogisticRegression(random_state=75, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current best score is: -32.01901772935724\n",
      "The best feature was t1_losses_alltime.  It scored 5.611100018475831\n",
      "Current best score is: 5.611100018475831\n",
      "The best feature was t2_win_percent_last_10.  It scored 11.127997276752339\n",
      "Current best score is: 11.127997276752339\n",
      "NO IMPROVEMENT\n",
      "FINAL BEST SCORE: 11.127997276752339\n"
     ]
    }
   ],
   "source": [
    "f_lr=get_best_features(pos_features, model,  df_train_filtered, current_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t2_win_percent_last_10', 't1_losses_alltime', 'winner_label', 't1_odds', 't2_odds']\n"
     ]
    }
   ],
   "source": [
    "print(f_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current best score is: -12.261964022635624\n",
      "The best feature was t1_wins_vs_t2.  It scored 34.165873463357855\n",
      "Current best score is: 34.165873463357855\n",
      "The best feature was t2_losses_last_5.  It scored 60.097907893308125\n",
      "Current best score is: 60.097907893308125\n",
      "The best feature was t1_losses_season.  It scored 106.61346749144504\n",
      "Current best score is: 106.61346749144504\n",
      "The best feature was t1_win_percent_season.  It scored 109.27608407019953\n",
      "Current best score is: 109.27608407019953\n",
      "The best feature was t2_wins_last_5.  It scored 128.6612151144792\n",
      "Current best score is: 128.6612151144792\n",
      "NO IMPROVEMENT\n",
      "FINAL BEST SCORE: 128.6612151144792\n"
     ]
    }
   ],
   "source": [
    "f_rf=get_best_features(pos_features, model,  df_train_filtered, current_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t2_wins_last_5', 't1_win_percent_season', 't1_losses_season', 't2_losses_last_5', 't1_wins_vs_t2', 'winner_label', 't1_odds', 't2_odds']\n"
     ]
    }
   ],
   "source": [
    "print(f_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current best score is: -79.48500001896443\n",
      "The best feature was t1_matches_season.  It scored 15.772090093861332\n",
      "Current best score is: 15.772090093861332\n",
      "NO IMPROVEMENT\n",
      "FINAL BEST SCORE: 15.772090093861332\n"
     ]
    }
   ],
   "source": [
    "model=MLPClassifier(random_state=75)\n",
    "f_nn=f_rf=get_best_features(pos_features, model,  df_train_filtered, current_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t1_matches_season', 'winner_label', 't1_odds', 't2_odds']\n"
     ]
    }
   ],
   "source": [
    "print(f_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's pick some hyperparameters\n",
    "model = tree.DecisionTreeClassifier(max_depth=10, min_samples_leaf=3, random_state=75)\n",
    "features = ['t1_wins_last_3', 't2_matches_alltime', 'winner_label', 't1_odds', 't2_odds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_pos = ['gini', 'entropy']\n",
    "max_depth_pos = [3, 6, 9, 12, None, 10]\n",
    "min_samples_leaf_pos = [1,2,3,4,5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.821078751411875 ['gini', 6, 2]\n",
      "66.96480219928249 ['gini', 6, 3]\n",
      "74.68854411482693 ['entropy', 12, 2]\n",
      "74.96142463876302 ['entropy', 12, 4]\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_features = []\n",
    "for c in criterion_pos:\n",
    "    for md in max_depth_pos:\n",
    "        for msl in min_samples_leaf_pos:\n",
    "            model = tree.DecisionTreeClassifier(max_depth=md, min_samples_leaf=msl, criterion=c, random_state=75)\n",
    "            possible_score=custom_cv_eval(df_train_filtered[features], model)\n",
    "            if possible_score > best_score:\n",
    "                best_score=possible_score\n",
    "                best_features=[c, md, msl]\n",
    "                best_model = model\n",
    "                print(f\"{best_score} {best_features}\")\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(max_depth=10, min_samples_leaf=3, random_state=75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.52979394547228\n"
     ]
    }
   ],
   "source": [
    "print(custom_cv_eval(df_train_filtered[features], model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
      "                       max_depth=12, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=4, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=75, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=75, max_iter=1000)\n",
    "features = ['t2_win_percent_last_10', 't1_losses_alltime', 'winner_label', 't1_odds', 't2_odds']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_pos = [1e-4, 1e-3, 1e-2]\n",
    "fit_intercept_pos = [True, False]\n",
    "solver_pos = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "max_iter = [10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.557676825909205 ['entropy', None, 5]\n",
      "11.127997276752339 ['entropy', None, 5]\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_features = 0\n",
    "for tp in tol_pos:\n",
    "    for fi in fit_intercept_pos:\n",
    "        for s in solver_pos:\n",
    "            for mi in max_iter:\n",
    "                model = LogisticRegression(random_state=75, tol=tp, fit_intercept=fi, solver=s, max_iter=mi)\n",
    "                possible_score=custom_cv_eval(df_train_filtered[features], model)\n",
    "                if possible_score > best_score:\n",
    "                    best_score=possible_score\n",
    "                    best_features=[c, md, msl]\n",
    "                    best_model = model\n",
    "                    print(f\"{best_score} {best_features}\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=75)\n",
    "features = ['t2_wins_last_5', 't1_win_percent_season', 't1_losses_season', 't2_losses_last_5', 't1_wins_vs_t2',\n",
    "            'winner_label', 't1_odds', 't2_odds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_pos=[10,50,100,150]\n",
    "criterion_pos=['gini', 'entropy']\n",
    "max_depth_pos=[None, 2,4,6,8,10]\n",
    "min_samples_leaf_pos=[1,2,3,4,5,6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.488971559339962 [10, 'gini', None, 1]\n",
      "83.33567705218252 [10, 'gini', None, 2]\n",
      "88.54367924927315 [10, 'gini', 2, 1]\n",
      "94.45318697450092 [10, 'gini', 2, 2]\n",
      "111.02744486691587 [10, 'gini', 2, 3]\n",
      "120.69470875257205 [10, 'gini', 6, 1]\n",
      "136.77540311682964 [50, 'gini', 6, 1]\n"
     ]
    }
   ],
   "source": [
    "best_score=0\n",
    "best_features=0\n",
    "for ne in n_estimators_pos:\n",
    "    for c in criterion_pos:\n",
    "        for md in max_depth_pos:\n",
    "            for msl in min_samples_leaf_pos:\n",
    "                model = RandomForestClassifier(random_state=75, n_estimators=ne, criterion=c, max_depth=md,\n",
    "                                              min_samples_leaf=msl)\n",
    "                possible_score=custom_cv_eval(df_train_filtered[features], model)\n",
    "                if possible_score > best_score:\n",
    "                    best_score=possible_score\n",
    "                    best_features=[ne, c, md, msl]\n",
    "                    best_model = model\n",
    "                    print(f\"{best_score} {best_features}\")                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=6, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "                       n_jobs=None, oob_score=False, random_state=75, verbose=0,\n",
      "                       warm_start=False)\n",
      "['t2_wins_last_5', 't1_win_percent_season', 't1_losses_season', 't2_losses_last_5', 't1_wins_vs_t2', 'winner_label', 't1_odds', 't2_odds']\n"
     ]
    }
   ],
   "source": [
    "print(best_model)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MLPClassifier(random_state=75) \n",
    "features = ['t1_matches_season', 'winner_label', 't1_odds', 't2_odds']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes_pos = [(50,50,50), (50,100,50), (100,)]\n",
    "activation_pos = ['tanh', 'relu', 'logistic', 'identity']\n",
    "solver_pos = ['lbfgs', 'sgd', 'adam']\n",
    "alpha_pos = [.0001, .001, .01]\n",
    "max_iter = [500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.4824612494733564 [(50, 50, 50), 'tanh', 'sgd', 0.001, 500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.62378994273036 [(50, 50, 50), 'logistic', 'sgd', 0.0001, 500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:151: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:151: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-419-327c5161c85c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                     model = MLPClassifier(random_state=75, hidden_layer_sizes=hls, activation=a, solver=s, alpha=al,\n\u001b[0;32m      9\u001b[0m                                           max_iter=mi)\n\u001b[1;32m---> 10\u001b[1;33m                     \u001b[0mpossible_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_cv_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_filtered\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mpossible_score\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                         \u001b[0mbest_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpossible_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-359-49f870c50643>\u001b[0m in \u001b[0;36mcustom_cv_eval\u001b[1;34m(df, m)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#print(f\"{count}: {get_ev_from_df(ev_prepped_df, print_stats = False)}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mrunning_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunning_total\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mget_ev_from_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mev_prepped_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrunning_total\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-192-c805999a4ae7>\u001b[0m in \u001b[0;36mget_ev_from_df\u001b[1;34m(ev_df, print_stats)\u001b[0m\n\u001b[0;32m     95\u001b[0m                     \u001b[0mnum_even_losses\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[0mprofit_per_bet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprofit\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum_bets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m     \u001b[0mprofit_per_match\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprofit\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum_matches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "best_score=0\n",
    "best_features=0\n",
    "for hls in hidden_layer_sizes_pos:\n",
    "    for a in activation_pos:\n",
    "        for s in solver_pos:\n",
    "            for al in alpha_pos:\n",
    "                for mi in max_iter:\n",
    "                    model = MLPClassifier(random_state=75, hidden_layer_sizes=hls, activation=a, solver=s, alpha=al,\n",
    "                                          max_iter=mi)\n",
    "                    possible_score=custom_cv_eval(df_train_filtered[features], model)\n",
    "                    if possible_score > best_score:\n",
    "                        best_score=possible_score\n",
    "                        best_features=[hls, a, s, al, mi]\n",
    "                        best_model = model\n",
    "                        print(f\"{best_score} {best_features}\")                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['corona_virus_isolation', 't1_wins_season',\n",
    "       't1_losses_season', 't2_wins_season', 't2_losses_season',\n",
    "       't1_matches_season', 't2_matches_season', 't1_win_percent_season',\n",
    "       't2_win_percent_season', 't1_wins_alltime', 't1_losses_alltime',\n",
    "       't2_wins_alltime', 't2_losses_alltime', 't1_matches_alltime',\n",
    "       't2_matches_alltime', 't1_win_percent_alltime',\n",
    "       't2_win_percent_alltime', 't1_wins_last_3', 't1_losses_last_3',\n",
    "       't2_wins_last_3', 't2_losses_last_3', 't1_win_percent_last_3',\n",
    "       't2_win_percent_last_3', 't1_wins_last_5', 't1_losses_last_5',\n",
    "       't2_wins_last_5', 't2_losses_last_5', 't1_win_percent_last_5',\n",
    "       't2_win_percent_last_5', 't1_wins_last_10', 't1_losses_last_10',\n",
    "       't2_wins_last_10', 't2_losses_last_10', 't1_win_percent_last_10',\n",
    "       't2_win_percent_last_10', \n",
    "        't1_wins_vs_t2', 't1_losses_vs_t2',\n",
    "       't1_matches_vs_t2', 'winner_label', 't1_odds', 't2_odds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GradientBoostingClassifier(random_state=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_pos=[.05, 0.1, .2]\n",
    "n_estimators_pos=[25, 50, 100, 200]\n",
    "min_samples_leaf_pos=[1,2,3,4,5,6]\n",
    "max_depth_pos=[2,3,4,6,8,10,12,14,16]\n",
    "max_features_pos=['sqrt', 'log2', None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.526795644613316 [0.05, 25, 1, 2, 'sqrt']\n",
      "62.995473791949244 [0.05, 25, 1, 3, 'log2']\n",
      "65.8596120835575 [0.05, 25, 1, 4, 'log2']\n",
      "85.70035624211746 [0.05, 25, 1, 8, 'sqrt']\n",
      "92.64616006551302 [0.05, 25, 1, 10, 'sqrt']\n",
      "101.07348973671972 [0.05, 25, 1, 12, 'log2']\n"
     ]
    }
   ],
   "source": [
    "best_score=0\n",
    "best_features=0\n",
    "for lr in learning_rate_pos:\n",
    "    for ne in n_estimators_pos:\n",
    "        for ms in min_samples_leaf_pos:\n",
    "            for md in max_depth_pos:\n",
    "                for mf in max_features_pos:\n",
    "                    model=GradientBoostingClassifier(random_state=75, learning_rate=lr, n_estimators=ne,\n",
    "                                                    min_samples_leaf=ms, max_depth=md, max_features=mf)\n",
    "                    possible_score=custom_cv_eval(df_train_filtered[features], model)\n",
    "                    if possible_score > best_score:\n",
    "                        best_score=possible_score\n",
    "                        best_features=[lr, ne, ms, md, mf]\n",
    "                        best_model=model\n",
    "                        print(f\"{best_score} {best_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.2, loss='deviance', max_depth=16,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=6, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=75, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corona_virus_isolation', 't1_wins_season', 't1_losses_season', 't2_wins_season', 't2_losses_season', 't1_matches_season', 't2_matches_season', 't1_win_percent_season', 't2_win_percent_season', 't1_wins_alltime', 't1_losses_alltime', 't2_wins_alltime', 't2_losses_alltime', 't1_matches_alltime', 't2_matches_alltime', 't1_win_percent_alltime', 't2_win_percent_alltime', 't1_wins_last_3', 't1_losses_last_3', 't2_wins_last_3', 't2_losses_last_3', 't1_win_percent_last_3', 't2_win_percent_last_3', 't1_wins_last_5', 't1_losses_last_5', 't2_wins_last_5', 't2_losses_last_5', 't1_win_percent_last_5', 't2_win_percent_last_5', 't1_wins_last_10', 't1_losses_last_10', 't2_wins_last_10', 't2_losses_last_10', 't1_win_percent_last_10', 't2_win_percent_last_10', 't1_wins_vs_t2', 't1_losses_vs_t2', 't1_matches_vs_t2', 'winner_label', 't1_odds', 't2_odds']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
